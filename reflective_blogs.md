[Home](index.md) | [Manual Assessment Memo](manual_assessment_memo.md) | [Chatbot](chatbot.md) | [Procedure Video](procedure_video.md) | [Manual](manual.md) | [Reflective Blogs](reflective_blogs.md) 

# Table of Contents 
1. [Wayback Analysis](#wayback-analysis)
2. [Quick Start Guide](#quick-start-guide)
3. [Reflective Blog 3](#reflective-blog-3)
4. [Reflective Blog 4](#reflective-blog-4)
5. [Reflective Blog 5](#reflective-blog-5)
6. [Reflective Blog 6](#reflective-blog-6)
7. [Reflective Blog 7](#reflective-blog-7)

# Wayback Analysis

For my Wayback Analysis I chose the New York Times’s website. I think websites typically aim to have organized and simple layouts with concise titles, labels, and links. With the main interest being to not overwhelm or confuse users. Both versions of the New York Time website had these elements, but there were a couple of differences I noticed.

I chose a screenshot from August 29, 2005 to observe and the most unusual thing for me was how compact the information was on the page. Screen formats were typically narrower at the time, so it can be expected, but it is a big contrast to the current website where information is spread out far more. 
Another difference was the use of pictures on the current website. For each subsection, like sports or entertainment, at least one article is accompanied by a photo associated with the title. To add on, the current website also has brief descriptions or summaries for some of the articles. Relying on the idea that as the reader, if you want more details you’ll click on the link, in other words the title, provided. Whereas, in the 2005 version, few photos were used and even fewer descriptions were provided. They relied heavily on using captivating titles to draw readers in. 

Overall, the only out of ordinary thing I found for the website, in comparison to now, was the compact format of information and less use of visual aids. All the information was crammed into a tighter space making it feel a bit more cluttered then I am currently used to. With all that said, I personally enjoyed the blast from the past as a modern user. It was cool to be reminded how certain websites looked back in the early 2000s. 

---
# Quick Start Guide
[ENC 4265_ Quick-Start Guide.pdf](https://github.com/user-attachments/files/18612762/ENC.4265_.Quick-Start.Guide.pdf)

---
# Reflective Blog 3


---
# Reflective Blog 4
Prompt C:
For this reflective blog, I asked Copilot to generate a project plan for a developing cybersecurity system. I plan on going into cybersecurity when I graduate and hope to be a project manager one day. Project managers are tasked with multiple things and are responsible for lots of documentation. 

The original prompt I asked was, “Imagine you are a project manager. Could you provide an example of a project plan for a cybersecurity system being developed?”, to which the AI presented a rough outline of a project plan. The project plan was broken into nine categories, Project Overview, Project Goals and Objectives, Project Scope, Key Deliverables, Timeline and Milestones, Budget, Risk Management, Stakeholders (Team members), and Communication Plan. Each category had vague examples or descriptions to correspond with it. In conclusion, the results were a good example of a potential project plan, but there were still things that could be improved. 

The first improvement needed is vital categories not present in the original project plan, which include ‘Quality Criteria’, ‘Evaluating Criteria’, and ‘Procurement/Contracting Plan’. The first two would be used to help guide the testing stage of the system to give a clear outline of what the system should accomplish and what to do if the goals are not accomplished. Time was allocated in the ‘Timeline and Milestone’ category in the plan for testing, but no criteria were given for it. The last category represents what outside resources need to be acquired to develop the system, like supplies or other manual labor. Another improvement needed is less vague language and more specifics. Like stating the responsibilities of team members and what is included in each expense category. 

Therefore, my follow-up prompt was, “Could you include a ‘Quality Criteria’, ‘Evaluating Criteria’, and 'Procurement or Contracting Plan' in it. Plus please be more specific, like stating the responsibilities of team members and what is included in each expense category.” The results were improved versions of the original project plan, taking note of my suggestions. I wouldn’t say it is perfect, but I think it is a great draft.

To save space on the page I have included my questions and Copilot's responses in this document, [Copilot Responses.pdf](https://github.com/user-attachments/files/18897954/Copilot.Responses.pdf)


AI Check: I used Grammarly to improve my assignment and Copilot for the assignment itself.

---
# Reflective Blog 5
Prompt R: 
After chatting with a couple of chatbots for the ChatBot assignment, I found that there were many common characteristics between the different AI algorithms. They seem designed to answer most questions that don’t rely on feelings, opinions, or real-time events. The algorithms I used were, ChatGPT, Nova Pro, and Bing through Boodle Box. I decided to phrase my question as if I were talking/texting a friend to test how the algorithm responds. I used the question, “Omg queen! How many presidents in America were democrats? I literally have no clue?” 

All three replies were simple and lacked ‘personality’, stating there were 16 democratic presidents and then giving me the full list of presidents. The data used for all three responses seemed to come from the internet, with all algorithms displaying a ‘searching the web…’ statement before their response. However, I recalled a time when using ChaGPT on my phone where my question was also phrased as if I was texting/talking to a friend and the response mimicked the way I had phrased my question, rather than lacking ‘personality’ as it did on BoodleBox, so I asked the same question to ChatGPT on my phone and got a very different response. The response started with the statement, “Haha! I love the energy.” then proceeded to say “16 Democratic presidents..” and at the end asked if I wanted the whole list. I responded with “Yes pls” and the follow-up reply included the statements “Of course, queen!” and “Let me know if you need any tea on any of them!” 

Therefore, I think the data used to train them allows them to discern our emotions to craft a response it feels we would best understand, which can include mimicking the way we phrase our statements. However, I think some are algorithms are authorized to mimic us, like ChatGPT, while others are to remain without ‘personality’ like Bing and Nova Pro. I think overall AI does a good job at answering the questions given to it, but it has its times when it doesn’t give the exact answer you are looking for and you have to rephrase the question or urge it to give more information. For example, me having to respond, “Yes pls” to get the entire list of presidents rather than just accepting “16 Democratic presidents”. I think over time AI will just become more and more knowledgeable and understand human patterns much better, it may even predict what we could be thinking before we even ask.

AI Check: I used Grammarly to improve my assignment.

---
# Reflective Blog 6
Prompt K:
I asked ChatGPT to generate a Manual Assessment Memo for a Lego Flowerpot. For reference, the manual can be found here, <https://www.manua.ls/lego/flowerpot-40588/manual>. My first prompt was simple to just get an idea of what the AI thought I wanted when asking for a Manual Assessment Memo. I asked, “Write a Manual Assessment Memo for the Lego Flowerpot 40588 Manual.” ChatGPT assumed I was taking on the role of a product development team member working for LEGO. The response was thorough including an overview, general observations from throughout the manual, areas for improvements, and a conclusion. For the general observations, the AI included observations about the cover page, layout, structure, instructions quality, complexity and flow of the steps, build experience, feedback, and error free construction. The AI identified a lot of strengths for the Manual with a few recommendations for it, concluding the manual was, “ an excellent guide for builders of varying experience levels.”

Comparing the AI’s assessment to the rubric I followed for my Manual Mem Assessment for a HP laptop manual I would say the AI did a great job. It described the effective strengths of the manual and explained why these strengths are effective. Then included a sections for recommendations.The layout was a bit different, but it concluded a to, from, date, and subject at the top. 

To save space on the page I have included my question and ChatGPT's responses in this document, [AI Generative Manual Memo Assessment Conversation.pdf](https://github.com/user-attachments/files/19237502/AI.Generative.Manual.Memo.Assessment.Conversation.pdf).

AI Check: I used Grammarly to improve my assignment and ChatGPT to complete the assignment.

---
# Reflective Blog 7
Prompt F:
The deliverable I decided to reflect on is the Manual Assessment Memo. I would say it wasn’t difficult, but rather intimidating because I had never done one before. I struggled with finding weaknesses for the manual because I felt it had many strengths and is a pretty good example of how a manual should be done, but as I spent more time examining it I found some areas that could be improved. This assignment made me realize that I focus more on the positive aspects of something rather than the negative, but I enjoy how brief and simple technical communication is. I would improve my assignment based on feedback by using bullet points and making sure my thoughts are conveyed better. 


